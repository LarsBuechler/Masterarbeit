{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv as csv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from matplotlib.lines import Line2D\n",
    "import alphashape\n",
    "import random\n",
    "import json\n",
    "import math\n",
    "\n",
    "from scipy.stats import poisson\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import geopandas as gpd\n",
    "import pyproj\n",
    "from pyproj import Proj, transform, CRS\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation der Bevölkerungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Einwohner_Data/Bevoelkerung100M.csv'\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Merkmale, die gefiltert werden sollen\n",
    "list_of_merkmale = [' INSGESAMT', 'ALTER_KURZ', 'GESCHLECHT', 'ALTER_10JG']\n",
    "\n",
    "# Filtern des DataFrames df nach den angegebenen Merkmalen\n",
    "df_einwohner = df[df['Merkmal'].isin(list_of_merkmale)]\n",
    "\n",
    "# Erstellen eines leeren DataFrames für die Bezeichnungen\n",
    "df_bezeichnungen = pd.DataFrame()\n",
    "\n",
    "# Liste der Spaltennamen für die Bezeichnungen\n",
    "bezeichnungen = ['Merkmal', 'Auspraegung_Text', 'Auspraegung_Code']\n",
    "\n",
    "# Extrahieren der Bezeichnungsinformationen aus df_einwohner\n",
    "df_bezeichnungen = df_einwohner[bezeichnungen]\n",
    "\n",
    "# Entfernen von Duplikaten\n",
    "df_bezeichnungen.drop_duplicates(inplace=True)\n",
    "\n",
    "# Sortieren der Bezeichnungen nach 'Merkmal' und 'Auspraegung_Code'\n",
    "df_bezeichnungen.sort_values(by=['Merkmal', 'Auspraegung_Code'], inplace=True)\n",
    "\n",
    "# Zurücksetzen des Indexes\n",
    "df_bezeichnungen.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Speichern der Bezeichnungen als CSV-Datei\n",
    "df_bezeichnungen.to_csv('./Einwohner_Data/Bezeichnungen.csv', index=False)\n",
    "\n",
    "# Erstellen einer neuen Spalte 'Attribute' durch Kombination von 'Merkmal' und 'Auspraegung_Code'\n",
    "df_einwohner['Attribute'] = df_einwohner['Merkmal'] + '_' + df_einwohner['Auspraegung_Code'].astype(str)\n",
    "\n",
    "# Entfernen nicht benötigter Spalten\n",
    "df_einwohner.drop(columns=['Merkmal', 'Auspraegung_Code', 'Auspraegung_Text', 'Gitter_ID_100m', 'Anzahl_q'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df_einwohner.pivot(index=['Gitter_ID_100m_neu'], columns=['Attribute'], values='Anzahl').reset_index()\n",
    "pivot_df.columns.name = None\n",
    "pivot_df = pivot_df.fillna(0)\n",
    "pivot_df.rename(columns={' INSGESAMT_0': 'INSGESAMT_0'}, inplace=True)\n",
    "pivot_df.to_csv('./Einwohner_Data/Einwohner_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der bereinigten Einwohnerdaten aus einer CSV-Datei\n",
    "df_geo = pd.read_csv('./Einwohner_Data/Einwohner_Cleaned.csv')\n",
    "\n",
    "# Extrahieren von Informationen aus den Gitter-IDs mithilfe von regulären Ausdrücken\n",
    "pattern = re.compile(r'N(\\d+)E(\\d+)')\n",
    "df_geo[['origin_n', 'origin_e']] = df_geo['Gitter_ID_100m_neu'].str.extract(pattern).astype(int)\n",
    "\n",
    "# Setzen fester Werte für crs_code und resolution\n",
    "df_geo['crs_code'] = 3035\n",
    "df_geo['resolution'] = 100\n",
    "\n",
    "# Erstellen von Geometrien (Quadraten) für jedes Gitter\n",
    "df_geo['geometry'] = [Polygon([(e, n), (e + 100, n), (e + 100, n - 100), (e, n - 100)]) for e, n in zip(df_geo['origin_e'], df_geo['origin_n'])]\n",
    "\n",
    "# Speichern der Geodaten als CSV-Datei\n",
    "df_geo.to_csv('./GeoDaten/GeoDataFrame_Bevoelkerung.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df_geo, geometry='geometry', crs=CRS(f\"EPSG:{df_geo['crs_code'].iloc[0]}\"))\n",
    "gdf.to_file('./GeoDaten/Deutschland_Raster.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin, vmax = 0, 20\n",
    "# Plotte das GeoDataFrame mit Farbhervorhebung der 'INSGESAMT'-Werte und angepasster Skala\n",
    "gdf.plot(column='INSGESAMT_0', cmap='viridis', legend=True, figsize=(10, 10), vmin=vmin, vmax=vmax)\n",
    "plt.title('Farbliche Hervorhebung der INSGESAMT-Werte')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtern der Bevölkerungsdaten nach den NUTS-Grenzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('./99_Old/GeoDaten/Deutschland_Raster_Bevoelkerung.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Geodaten der Bundesländer aus einer GeoPackage-Datei\n",
    "geo_bundesländer = gpd.read_file('./99_Old/GeoDaten/DE_NUTS5000.gpkg')\n",
    "\n",
    "# Filtern der Geodaten, um nur die Einträge für Würzburg zu erhalten\n",
    "geo_würzburg = geo_bundesländer[geo_bundesländer['NUTS_NAME'].str.contains('Würzburg')]\n",
    "\n",
    "# Konvertieren der Koordinatenreferenzsystem (CRS) von geo_würzburg, um es dem CRS von gdf anzupassen\n",
    "geo_würzburg = geo_würzburg.to_crs(gdf.crs)\n",
    "\n",
    "# Speichern der gefilterten und konvertierten Geodaten als GeoPackage-Datei\n",
    "geo_würzburg.to_file('./Wuerzburg_Data/geo_wuerzburg.gpkg', driver='GPKG')\n",
    "\n",
    "# Erstellen einer Maske, um die überlappenden Geometrien zwischen gdf und geo_würzburg zu finden\n",
    "mask_overlapping = gdf.geometry.intersects(geo_würzburg.unary_union)\n",
    "\n",
    "# Filtern von gdf, um nur die überlappenden Geometrien zu behalten\n",
    "gdf_würzburg = gdf[mask_overlapping]\n",
    "\n",
    "# Speichern der gefilterten Geodaten als GeoPackage-Datei\n",
    "gdf_würzburg.to_file('./Wuerzburg_Data/Raster_Wuerzburg.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Geodaten der Bundesländer aus einer GeoPackage-Datei\n",
    "geo_bundesländer = gpd.read_file('./GeoDaten/DE_NUTS5000.gpkg')\n",
    "\n",
    "# Filtern der Geodaten, um nur die Einträge für Frankfurt am Main zu erhalten\n",
    "geo_frankfurt = geo_bundesländer[geo_bundesländer['NUTS_NAME'].str.contains('Frankfurt am Main')]\n",
    "\n",
    "# Konvertieren des Koordinatenreferenzsystems (CRS) von geo_frankfurt zu EPSG:3035\n",
    "geo_frankfurt = geo_frankfurt.to_crs('3035')\n",
    "\n",
    "# Speichern der gefilterten und konvertierten Geodaten als GeoPackage-Datei\n",
    "geo_frankfurt.to_file('./Frankfurt_Data/geo_frankfurt.gpkg', driver='GPKG')\n",
    "\n",
    "# Erstellen einer Maske, um die überlappenden Geometrien zwischen gdf und geo_frankfurt zu finden\n",
    "mask_overlapping = gdf.geometry.intersects(geo_frankfurt.unary_union)\n",
    "\n",
    "# Filtern von gdf, um nur die überlappenden Geometrien zu behalten\n",
    "gdf_frankfurt = gdf[mask_overlapping]\n",
    "\n",
    "# Speichern der gefilterten Geodaten als GeoPackage-Datei\n",
    "gdf_frankfurt.to_file('./Frankfurt_Data/Raster_Frankfurt.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einlesen der Geodaten der Bundesländer aus einer GeoPackage-Datei\n",
    "geo_bundesländer = gpd.read_file('./GeoDaten/DE_NUTS5000.gpkg')\n",
    "\n",
    "# Filtern der Geodaten, um nur die Einträge für Donnersbergkreis zu erhalten\n",
    "geo_donner = geo_bundesländer[geo_bundesländer['NUTS_NAME'].str.contains('Donnersbergkreis')]\n",
    "\n",
    "# Konvertieren des Koordinatenreferenzsystems (CRS) von geo_donner zu EPSG:3035\n",
    "geo_donner = geo_donner.to_crs('3035')\n",
    "\n",
    "# Speichern der gefilterten und konvertierten Geodaten als GeoPackage-Datei\n",
    "geo_donner.to_file('./Donner_Data/geo_donner.gpkg', driver='GPKG')\n",
    "\n",
    "# Erstellen einer Maske, um die überlappenden Geometrien zwischen gdf und geo_donner zu finden\n",
    "mask_overlapping = gdf.geometry.intersects(geo_donner.unary_union)\n",
    "\n",
    "# Filtern von gdf, um nur die überlappenden Geometrien zu behalten\n",
    "gdf_donner = gdf[mask_overlapping]\n",
    "\n",
    "# Speichern der gefilterten Geodaten als GeoPackage-Datei\n",
    "gdf_donner.to_file('./Donner_Data/Raster_Donner.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zuweisung der Bevölkerungsdaten auf die Gebäudedaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buildings = pd.read_csv('./Donner_Data/Donner-Buildings.csv')\n",
    "gdf_würzburg = gpd.read_file('./Donner_Data/Raster_Donner.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gebäude einem Raster zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_polygon_id(point, polygons):\n",
    "    \"\"\"\n",
    "    Findet die ID des nächstgelegenen Polygons zu einem gegebenen Punkt.\n",
    "\n",
    "    Args:\n",
    "        point (GeoDataFrame): Ein GeoDataFrame, der den Punkt enthält.\n",
    "        polygons (GeoDataFrame): Ein GeoDataFrame, das die Polygone enthält.\n",
    "\n",
    "    Returns:\n",
    "        int: Die ID des nächstgelegenen Polygons.\n",
    "    \"\"\"\n",
    "    # Berechnung der Distanz zwischen dem Punkt und allen Polygonen\n",
    "    distances = polygons.distance(point.geometry)\n",
    "    \n",
    "    # Sortieren der Distanzen und Abrufen des Indexes (ID) des nächstgelegenen Polygons\n",
    "    nearest_polygon_id = distances.sort_values().index[0]\n",
    "    \n",
    "    return nearest_polygon_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen eines GeoDataFrame für die Gebäude mit den Koordinaten und dem CRS EPSG:4326\n",
    "gdf_buildings = gpd.GeoDataFrame(df_buildings, geometry=gpd.points_from_xy(df_buildings['lon'], df_buildings['lat']), crs=CRS(\"EPSG:4326\"))\n",
    "\n",
    "# Umprojizieren des GeoDataFrames der Gebäude in das CRS von gdf_würzburg\n",
    "gdf_buildings = gdf_buildings.to_crs(gdf_würzburg.crs)\n",
    "\n",
    "# Initialisieren der 'Raster_ID' Spalte mit -1\n",
    "df_buildings['Raster_ID'] = -1\n",
    "\n",
    "# Iterieren über jede Zeile im GeoDataFrame der Gebäude\n",
    "for index, building in gdf_buildings.iterrows():\n",
    "    # Erstellen einer Maske, um zu prüfen, ob das Gebäude mit einem Polygon in gdf_würzburg überlappt\n",
    "    mask_overlapping = gdf_würzburg.intersects(building['geometry'])\n",
    "    \n",
    "    if any(mask_overlapping):\n",
    "        # Wenn das Gebäude mit einem Polygon überlappt, setze die 'Raster_ID' auf die Index des ersten überlappenden Polygons\n",
    "        df_buildings.loc[index, 'Raster_ID'] = gdf_würzburg[mask_overlapping].index[0]\n",
    "    else:\n",
    "        # Wenn das Gebäude mit keinem Polygon überlappt, finde das nächstgelegene Polygon und setze die 'Raster_ID'\n",
    "        df_buildings.loc[index, 'Raster_ID'] = find_nearest_polygon_id(building, gdf_würzburg)\n",
    "\n",
    "# Erstellen eines neuen GeoDataFrames mit den Gebäuden und den zugewiesenen Raster-IDs\n",
    "gdf_buildings_with_raster = gpd.GeoDataFrame(df_buildings, geometry=gdf_buildings['geometry'], crs=gdf_würzburg.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zählen der eindeutigen Werte in der Spalte 'Raster_ID'\n",
    "value_counts = gdf_buildings_with_raster['Raster_ID'].value_counts()\n",
    "\n",
    "# Sortieren der Ergebnisse nach den Werten in absteigender Reihenfolge\n",
    "sorted_value_counts = value_counts.sort_values(ascending=False)\n",
    "\n",
    "# Ausgabe der sortierten Ergebnisse\n",
    "print(\"Sortierte Wertezählungen:\")\n",
    "print(sorted_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jedem Gebäude die Bevölkerung zuweisen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisieren neuer Spalten in gdf_buildings_with_raster\n",
    "gdf_buildings_with_raster['Einwohner'] = 0\n",
    "gdf_buildings_with_raster['Alter'] = {} \n",
    "gdf_buildings_with_raster['Geschlecht'] = {} \n",
    "\n",
    "# Sammeln eindeutiger Raster-IDs\n",
    "unique_raster_indices = gdf_buildings_with_raster['Raster_ID'].unique()\n",
    "\n",
    "# Durchlaufe alle eindeutigen Raster-IDs\n",
    "for raster_index in unique_raster_indices:\n",
    "    # Anzahl der Einwohner aus dem Raster extrahieren\n",
    "    einwohner_aus_raster = gdf_würzburg.at[raster_index, 'INSGESAMT_0']\n",
    "\n",
    "    # Altersverteilung aus dem Raster extrahieren\n",
    "    alter_dict_raster = {\n",
    "        '0-10':  int(gdf_würzburg.at[raster_index, 'ALTER_10JG_1']),\n",
    "        '10-20': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_2']),\n",
    "        '20-30': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_3']),\n",
    "        '30-40': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_4']), \n",
    "        '40-50': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_5']), \n",
    "        '50-60': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_6']),\n",
    "        '60-70': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_7']), \n",
    "        '70-80': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_8']),\n",
    "        '80+':   int(gdf_würzburg.at[raster_index, 'ALTER_10JG_9'])\n",
    "    }\n",
    "\n",
    "    # Geschlechtsverteilung aus dem Raster extrahieren\n",
    "    geschlecht_dict_raster = {\n",
    "        'maennlich': gdf_würzburg.at[raster_index, 'GESCHLECHT_1'],\n",
    "        'weiblich':  gdf_würzburg.at[raster_index, 'GESCHLECHT_2']\n",
    "    }\n",
    "\n",
    "    # Durchlaufe alle Gebäude im aktuellen Raster\n",
    "    for index, building in gdf_buildings_with_raster[gdf_buildings_with_raster['Raster_ID'] == raster_index].iterrows():\n",
    "        \n",
    "        # Initialisieren von Dictionaries für Alters- und Geschlechtsverteilungen im Gebäude\n",
    "        alter_dict_geb = {\n",
    "            '0-10': 0,\n",
    "            '10-20': 0,\n",
    "            '20-30': 0,\n",
    "            '30-40': 0, \n",
    "            '40-50': 0, \n",
    "            '50-60': 0,\n",
    "            '60-70': 0, \n",
    "            '70-80': 0,\n",
    "            '80+': 0\n",
    "        }\n",
    "\n",
    "        geschlecht_dict_geb = {\n",
    "            'maennlich': 0,\n",
    "            'weiblich': 0\n",
    "        }\n",
    "\n",
    "        # Zuweisung einer zufälligen Anzahl von Einwohnern zum Gebäude\n",
    "        if index == gdf_buildings_with_raster[gdf_buildings_with_raster['Raster_ID'] == raster_index].index.max():\n",
    "            random_einwohner = einwohner_aus_raster\n",
    "        else:\n",
    "            random_einwohner = np.random.choice(int(einwohner_aus_raster))\n",
    "\n",
    "        random_einwohner = int(random_einwohner)\n",
    "        alter_count = random_einwohner\n",
    "        geschlecht_count = random_einwohner\n",
    "\n",
    "        # Zufällige Zuweisung von Alter\n",
    "        for key in np.random.permutation(list(alter_dict_raster.keys())):\n",
    "            if alter_count == 0:\n",
    "                break\n",
    "            if alter_dict_raster[key] > 0:\n",
    "                if alter_count <= alter_dict_raster[key]:\n",
    "                    alter_dict_geb[key] = alter_count\n",
    "                    alter_dict_raster[key] -= alter_count\n",
    "                    alter_count = 0\n",
    "                else:\n",
    "                    alter_dict_geb[key] = alter_dict_raster[key]\n",
    "                    alter_count -= alter_dict_raster[key]\n",
    "                    alter_dict_raster[key] = 0\n",
    "\n",
    "        # Falls noch ein Restwert vorhanden ist, zufällig einem Altersbereich zuweisen\n",
    "        if alter_count != 0:\n",
    "            alter_dict_geb[random.choice(list(alter_dict_geb.keys()))] += alter_count\n",
    "\n",
    "        # Zufällige Zuweisung von Geschlecht\n",
    "        for key in np.random.permutation(list(geschlecht_dict_raster.keys())):\n",
    "            if geschlecht_count == 0:\n",
    "                break\n",
    "            if geschlecht_dict_raster[key] > 0:\n",
    "                if geschlecht_count <= geschlecht_dict_raster[key]:\n",
    "                    geschlecht_dict_geb[key] = geschlecht_count\n",
    "                    geschlecht_dict_raster[key] -= geschlecht_count\n",
    "                    geschlecht_count = 0\n",
    "                else:\n",
    "                    geschlecht_dict_geb[key] = geschlecht_dict_raster[key]\n",
    "                    geschlecht_count -= geschlecht_dict_raster[key]\n",
    "                    geschlecht_dict_raster[key] = 0\n",
    "\n",
    "        # Falls noch ein Restwert vorhanden ist, zufällig einem Geschlecht zuweisen\n",
    "        if geschlecht_count != 0:\n",
    "            geschlecht_dict_geb[random.choice(list(geschlecht_dict_geb.keys()))] += geschlecht_count\n",
    "\n",
    "        # Aktualisieren der Gebäudeinformationen im GeoDataFrame\n",
    "        gdf_buildings_with_raster.at[index, 'Einwohner'] = random_einwohner\n",
    "        gdf_buildings_with_raster.loc[index, 'Alter'] = [alter_dict_geb]\n",
    "        gdf_buildings_with_raster.loc[index, 'Geschlecht'] = [geschlecht_dict_geb]\n",
    "        einwohner_aus_raster -= random_einwohner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buildings_with_raster['Alter'] = gdf_buildings_with_raster['Alter'].apply(json.dumps)\n",
    "gdf_buildings_with_raster['Geschlecht'] = gdf_buildings_with_raster['Geschlecht'].apply(json.dumps)\n",
    "\n",
    "gdf_buildings_with_raster.to_file('./Donner_Data/Buildings_Raster_Demographie.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zuweisung der nächsten Apotheke zu den Gebäuden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zu entfernende Spalten definieren\n",
    "columns_to_drop = ['name', 'building', 'addr:street', 'addr:housenumber', 'addr:postcode']\n",
    "\n",
    "# Einlesen der Gebäudedaten mit Raster- und Demographiedaten aus einer GeoPackage-Datei\n",
    "bevölkerungs_gdf = gpd.read_file('./Donner_Data/Buildings_Raster_Demographie.gpkg')\n",
    "\n",
    "# Umwandeln der Spalten 'Alter' und 'Geschlecht' in Dictionaries\n",
    "bevölkerungs_gdf['Alter'] = bevölkerungs_gdf['Alter'].apply(json.loads)\n",
    "bevölkerungs_gdf['Geschlecht'] = bevölkerungs_gdf['Geschlecht'].apply(json.loads)\n",
    "\n",
    "# Entfernen der definierten Spalten aus dem DataFrame\n",
    "bevölkerungs_gdf.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Einlesen der Apothekendaten aus einer CSV-Datei\n",
    "pharmacy_df = pd.read_csv('./Donner_Data/Donner-Apotheken.csv')\n",
    "\n",
    "# Erstellen eines GeoDataFrame für die Apotheken mit den Koordinaten und dem CRS EPSG:4326\n",
    "pharmacy_gdf = gpd.GeoDataFrame(pharmacy_df, geometry=gpd.points_from_xy(pharmacy_df['lon'], pharmacy_df['lat']), crs=CRS(\"EPSG:4326\"))\n",
    "\n",
    "# Umprojizieren des GeoDataFrames der Apotheken in das CRS von bevölkerungs_gdf\n",
    "pharmacy_gdf = pharmacy_gdf.to_crs(bevölkerungs_gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_point_with_cKDTree(lon, lat, points_df):\n",
    "    \"\"\"\n",
    "    Findet den nächstgelegenen Punkt in einem DataFrame basierend auf Längen- und Breitengrad.\n",
    "    \n",
    "    Parameter:\n",
    "    lon (float): Längengrad des Referenzpunkts.\n",
    "    lat (float): Breitengrad des Referenzpunkts.\n",
    "    points_df (DataFrame): DataFrame, das die Punkte mit den Spalten 'lon' und 'lat' enthält.\n",
    "\n",
    "    Rückgabe:\n",
    "    Series: Die Zeile des nächstgelegenen Punkts im DataFrame.\n",
    "    \"\"\"\n",
    "    # Extrahiere die Koordinaten aus dem DataFrame als NumPy-Array\n",
    "    coordinates = np.array(points_df[['lon', 'lat']])\n",
    "    \n",
    "    # Erstelle ein NumPy-Array für den Referenzpunkt\n",
    "    reference_point = np.array([lon, lat])\n",
    "    \n",
    "    # Berechne die paarweisen quadrierten Distanzen (schneller für cKDTree)\n",
    "    distances = np.sum((coordinates - reference_point)**2, axis=1)\n",
    "    \n",
    "    # Finde den Index der minimalen Distanz\n",
    "    idx = np.argmin(distances)\n",
    "\n",
    "    # Gib die Zeile des nächstgelegenen Punkts zurück\n",
    "    return points_df.iloc[idx]  # Verwende iloc für schnellere Integer-Indexierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_to_nearest_pharmacy(row):\n",
    "    \"\"\"\n",
    "    Findet die nächstgelegene Apotheke zu einem gegebenen Punkt und gibt die ID der nächstgelegenen Apotheke zurück.\n",
    "\n",
    "    Parameter:\n",
    "    row (Series): Eine Zeile des DataFrame, die die Koordinaten 'lon' und 'lat' des Punkts enthält.\n",
    "\n",
    "    Rückgabe:\n",
    "    str: Die ID der nächstgelegenen Apotheke.\n",
    "    \"\"\"\n",
    "    # Finde die nächstgelegene Apotheke zu den Koordinaten des gegebenen Punkts\n",
    "    nearest_pharmacy = find_nearest_point_with_cKDTree(row.lon, row.lat, pharmacy_gdf)\n",
    "    \n",
    "    # Gib die ID der nächstgelegenen Apotheke zurück\n",
    "    return nearest_pharmacy['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisieren der Spalte 'assigned_pharmacy' in bevölkerungs_gdf mit 0\n",
    "bevölkerungs_gdf['assigned_pharmacy'] = 0 \n",
    "\n",
    "# Aktivieren der Fortschrittsanzeige für Pandas-Operationen\n",
    "tqdm.pandas()\n",
    "\n",
    "# Durchlaufen aller Zeilen im DataFrame bevölkerungs_gdf\n",
    "for index, row in tqdm(bevölkerungs_gdf.iterrows(), total=len(bevölkerungs_gdf)):\n",
    "    # Berechnen der nächstgelegenen Apotheke für die aktuelle Zeile\n",
    "    pharmacy = calculate_distance_to_nearest_pharmacy(row)\n",
    "    # Zuweisen der ID der nächstgelegenen Apotheke zur Spalte 'assigned_pharmacy'\n",
    "    bevölkerungs_gdf.loc[index, 'assigned_pharmacy'] = pharmacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bevölkerungs_gdf['Alter'] = bevölkerungs_gdf['Alter'].apply(json.dumps)\n",
    "bevölkerungs_gdf['Geschlecht'] = bevölkerungs_gdf['Geschlecht'].apply(json.dumps)\n",
    "bevölkerungs_gdf.to_file('./Donner_Data/pharmacy_assigned.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nachfrage-Wahrscheinlichkeitsverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequenz von Apothekenbesuchen nach Altersverteilung\n",
    "# Die Kategorien der Besuchshäufigkeit sind:\n",
    "# \"Mehrmals in der Woche, Etwa einmal in der Woche, Zwei- bis dreimal im Monat, Einmal im Monat, Etwa einmal im Vierteljahr, Seltener, Nie, so gut wie nie\"\n",
    "\n",
    "# Dictionary mit der Verteilung der Apothekenbesuche nach Altersgruppen\n",
    "apotheken_besuch = {\n",
    "    '14-19': [0.002, 0.005, 0.023, 0.053, 0.18, 0.305, 0.432],  # Altersgruppe 14-19 Jahre\n",
    "    '20-29': [0.003, 0.009, 0.052, 0.131, 0.282, 0.326, 0.197],  # Altersgruppe 20-29 Jahre\n",
    "    '30-39': [0.006, 0.013, 0.098, 0.198, 0.287, 0.275, 0.123],  # Altersgruppe 30-39 Jahre\n",
    "    '40-49': [0.009, 0.014, 0.105, 0.21, 0.297, 0.258, 0.106],   # Altersgruppe 40-49 Jahre\n",
    "    '50-59': [0.008, 0.019, 0.116, 0.252, 0.308, 0.209, 0.087],  # Altersgruppe 50-59 Jahre\n",
    "    '60-69': [0.005, 0.037, 0.173, 0.277, 0.278, 0.159, 0.071],  # Altersgruppe 60-69 Jahre\n",
    "    '70+':   [0.011, 0.065, 0.252, 0.336, 0.196, 0.064, 0.075]   # Altersgruppe 70+ Jahre\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisieren des Dictionaries für die Altersverteilung\n",
    "alter_verteilung = {\n",
    "    \"10-20\": 0,\n",
    "    \"20-30\": 0,\n",
    "    \"30-40\": 0,\n",
    "    \"40-50\": 0,\n",
    "    \"50-60\": 0,\n",
    "    \"60-70\": 0,\n",
    "    \"70-80\": 0\n",
    "}\n",
    "\n",
    "# Extrahieren der Wahrscheinlichkeiten aus dem apotheken_besuch Dictionary und Umwandeln in ein NumPy Array\n",
    "prob_werte = list(apotheken_besuch.values())\n",
    "prob_arrays = np.stack(prob_werte)\n",
    "\n",
    "# Extrahieren der Schlüssel (Altersgruppen) aus dem alter_verteilung Dictionary\n",
    "keys = list(alter_verteilung.keys())\n",
    "\n",
    "# Initialisieren der Liste für die Wahrscheinlichkeiten\n",
    "probabilities = []\n",
    "\n",
    "# Berechnung der täglichen Besuchswahrscheinlichkeiten basierend auf den monatlichen Verteilungen\n",
    "for i in range(5):\n",
    "    for j in range(7):  \n",
    "        if i == 0:  # Mehrmals in der Woche\n",
    "            alter_verteilung[keys[j]] += prob_arrays[j][i] / 3.85714285714\n",
    "        elif i == 1:  # Etwa einmal in der Woche\n",
    "            alter_verteilung[keys[j]] += prob_arrays[j][i] / 7\n",
    "        elif i == 2:  # Zwei- bis dreimal im Monat\n",
    "            alter_verteilung[keys[j]] += prob_arrays[j][i] / 12\n",
    "        elif i == 3:  # Einmal im Monat\n",
    "            alter_verteilung[keys[j]] += prob_arrays[j][i] / 30\n",
    "        elif i == 4:  # Etwa einmal im Vierteljahr\n",
    "            alter_verteilung[keys[j]] += prob_arrays[j][i] / 90\n",
    "\n",
    "# Anpassung der Verteilung für die Altersgruppe \"10-20\"\n",
    "alter_verteilung[\"10-20\"] = alter_verteilung[\"10-20\"] / 2\n",
    "\n",
    "# Ausgabe der berechneten Altersverteilung\n",
    "print(alter_verteilung)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering der Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_city = gpd.read_file('./99_Old/Frankfurt/Raster_frankfurt.gpkg')\n",
    "geo_gdf = gpd.read_file('./Frankfurt_Data/geo_frankfurt.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren der Polygon-Koordinaten als Features für das Clustering\n",
    "# Erstellen eines NumPy-Arrays mit den Koordinaten der Zentroiden der Polygone\n",
    "X = np.array(gdf_city.geometry.apply(lambda polygon: [polygon.centroid.x, polygon.centroid.y]).tolist())\n",
    "\n",
    "# DBSCAN-Clustering durchführen\n",
    "# Initialisieren des DBSCAN-Algorithmus mit den Parametern eps und min_samples\n",
    "dbscan = DBSCAN(eps=100, min_samples=4)  # Anpassen von eps und min_samples je nach Bedarf\n",
    "\n",
    "# Anpassen des DBSCAN-Modells an die Daten\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Fügen Sie die Cluster-Zuordnung als neue Spalte zum GeoDataFrame hinzu\n",
    "# Die Cluster-Zuordnungen (labels_) werden als neue Spalte 'cluster' zum GeoDataFrame hinzugefügt\n",
    "gdf_city['cluster'] = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gruppieren nach Clustern und Polygone in MultiPolygone umwandeln\n",
    "# multi_polygons = gdf_city.groupby('cluster')['geometry'].apply(lambda x: MultiPolygon(list(x)))\n",
    "# # Ein neues GeoDataFrame erstellen mit den MultiPolygons\n",
    "# gdf_multi = gpd.GeoDataFrame(multi_polygons, geometry='geometry').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion, um einen Teil einer Colormap zu extrahieren\n",
    "def truncate_colormap(cmap, minval=0.5, maxval=1.0, n=100):\n",
    "    new_cmap = plt.cm.colors.LinearSegmentedColormap.from_list(\n",
    "        f'trunc({cmap.name},{minval:.2f},{maxval:.2f})', \n",
    "        cmap(np.linspace(minval, maxval, n)))\n",
    "    return new_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verwende den helleren Teil der cividis-Colormap\n",
    "cmap = truncate_colormap(plt.get_cmap('viridis'), 0.5, 1.0)\n",
    "\n",
    "# Berechne die 10 größten Cluster ohne Cluster -1\n",
    "top_clusters = gdf_city[gdf_city['cluster'] != -1]['cluster'].value_counts().nlargest(10).index\n",
    "\n",
    "# Anzahl der Polygone im Cluster -1\n",
    "outlier_count = len(gdf_city[gdf_city['cluster'] == -1])\n",
    "\n",
    "# Plotten der Boundary\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "geo_gdf.boundary.plot(ax=ax, color='gray', linewidth=0.5)\n",
    "\n",
    "# Plotten der Polygone mit Cluster-Färbung\n",
    "gdf_city[gdf_city['cluster'] != -1].plot(ax=ax, column='cluster', cmap=cmap, legend=False)\n",
    "gdf_city[gdf_city['cluster'] == -1].plot(ax=ax, color='purple', legend=False)  # Lila für Cluster -1\n",
    "\n",
    "# for cluster_id in gdf_multi[gdf_multi['cluster']!= -1].cluster:\n",
    "#     cluster_data = gdf_city[gdf_city['cluster'] == cluster_id]\n",
    "#     # Kombiniere alle Geometrien des Clusters zu einer einzigen Geometrie\n",
    "#     combined_geom = cluster_data.unary_union\n",
    "#     # Berechne die konvexe Hülle der kombinierten Geometrie\n",
    "#     if isinstance(combined_geom, (MultiPolygon, Polygon)):\n",
    "#         outer_boundary = combined_geom.convex_hull\n",
    "#         gpd.GeoSeries(outer_boundary).boundary.plot(ax=ax, color='lightgray', linewidth=0.4)\n",
    "#     else:\n",
    "#         for geom in combined_geom.geoms:\n",
    "#             gpd.GeoSeries(geom.convex_hull).boundary.plot(ax=ax, color='lightgray', linewidth=0.4)\n",
    "\n",
    "\n",
    "# Zentroiden und Anzahl der Polygone der 10 größten Cluster anzeigen\n",
    "for cluster_id in top_clusters:\n",
    "    cluster_data = gdf_city[gdf_city['cluster'] == cluster_id]\n",
    "    centroid = cluster_data.geometry.unary_union.centroid\n",
    "    polygon_count = len(cluster_data)\n",
    "    ax.plot(centroid.x, centroid.y, '^', color='red', markersize=6)\n",
    "    ax.text(centroid.x + 1800, centroid.y, str(polygon_count), fontsize=12, ha='center', va='center', color='black', fontweight='bold')  # Versetzte Position\n",
    "\n",
    "# Hinzufügen eines Farbbalkens\n",
    "norm = plt.Normalize(vmin=gdf_city['cluster'].min(), vmax=gdf_city['cluster'].max())\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax, shrink=0.65)  # Colormap kleiner machen\n",
    "cbar.set_label('ID der Cluster')\n",
    "# Text unterhalb der Farblegende hinzufügen\n",
    "plt.text(1.2, 0, f'Anzahl der Ausreiser: {outlier_count}', ha='right', va='center', transform=ax.transAxes, fontsize=10)\n",
    "\n",
    "\n",
    "# Add scale bar\n",
    "scalebar = ScaleBar(1, location='lower left', label='Maßstab')\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle eindeutigen Cluster-Werte und ihre Anzahl der Polygone ausgeben\n",
    "cluster_counts = gdf_city['cluster'].value_counts().sort_index()\n",
    "print(\"Cluster-Werte und ihre Anzahl der Polygone:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} Polygone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der Rauschpunkte (Clusterlabel -1)\n",
    "noise_points = gdf_city[gdf_city['cluster'] == -1]\n",
    "ax = noise_points.boundary.plot(color='red', alpha=0.5, figsize=(5, 5))\n",
    "plt.title('Noise')\n",
    "plt.xlabel('X-Achse')\n",
    "plt.ylabel('Y-Achse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_sum = 'INSGESAMT_0'\n",
    "sum_of_column = noise_points[column_to_sum].sum()\n",
    "\n",
    "print('Anzahl der Raster, die als Noise erkannt wurden: {}'.format(len(noise_points)))\n",
    "print(\"Sum of column '{}' in noise points: {}\".format(column_to_sum, sum_of_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gruppierung nach dem Cluster und Aggregierung der Punkte in jedem Cluster\n",
    "cluster_summary = gdf_city.groupby('cluster').agg(\n",
    "    Gitter_ID_100m_neu=('Gitter_ID_100m_neu', 'first'),  # Erster Wert von Gitter_ID_100m_neu im Cluster\n",
    "    count=('cluster', 'size'),  # Anzahl der Polygone in jedem Cluster\n",
    "    sum_INSGESAMT_0=('INSGESAMT_0', 'sum'),  # Summe der Gesamtbevölkerung im Cluster\n",
    "    sum_ALTER_10JG_1=('ALTER_10JG_1', 'sum'),  # Summe der Altersgruppe 0-10 Jahre im Cluster\n",
    "    sum_ALTER_10JG_2=('ALTER_10JG_2', 'sum'),  # Summe der Altersgruppe 10-20 Jahre im Cluster\n",
    "    sum_ALTER_10JG_3=('ALTER_10JG_3', 'sum'),  # Summe der Altersgruppe 20-30 Jahre im Cluster\n",
    "    sum_ALTER_10JG_4=('ALTER_10JG_4', 'sum'),  # Summe der Altersgruppe 30-40 Jahre im Cluster\n",
    "    sum_ALTER_10JG_5=('ALTER_10JG_5', 'sum'),  # Summe der Altersgruppe 40-50 Jahre im Cluster\n",
    "    sum_ALTER_10JG_6=('ALTER_10JG_6', 'sum'),  # Summe der Altersgruppe 50-60 Jahre im Cluster\n",
    "    sum_ALTER_10JG_7=('ALTER_10JG_7', 'sum'),  # Summe der Altersgruppe 60-70 Jahre im Cluster\n",
    "    sum_ALTER_10JG_8=('ALTER_10JG_8', 'sum'),  # Summe der Altersgruppe 70-80 Jahre im Cluster\n",
    "    sum_ALTER_10JG_9=('ALTER_10JG_9', 'sum'),  # Summe der Altersgruppe 80+ Jahre im Cluster\n",
    "    sum_GESCHLECHT_1=('GESCHLECHT_1', 'sum'),  # Summe der männlichen Bevölkerung im Cluster\n",
    "    sum_GESCHLECHT_2=('GESCHLECHT_2', 'sum'),  # Summe der weiblichen Bevölkerung im Cluster\n",
    "    crs_code=('crs_code', 'first'),  # Erster Wert von crs_code im Cluster\n",
    "    resolution=('resolution', 'first'),  # Erster Wert von resolution im Cluster\n",
    "    origin_n=('origin_n', 'first'),  # Erster Wert von origin_n im Cluster\n",
    "    origin_e=('origin_e', 'first'),  # Erster Wert von origin_e im Cluster\n",
    "    geometry=('geometry', lambda x: unary_union(x))  # Vereinigung der Polygone in jedem Cluster\n",
    ")\n",
    "\n",
    "# Umwandeln des aggregierten DataFrames in ein GeoDataFrame\n",
    "# Definieren eines neuen GeoDataFrames mit der aggregierten Geometrie und den Eigenschaften\n",
    "cluster_summary_gdf = gpd.GeoDataFrame(cluster_summary, geometry='geometry', crs=gdf_city.crs)\n",
    "\n",
    "# Zurücksetzen des Index, um den Cluster-Wert als Spalte beizubehalten\n",
    "cluster_summary_gdf = cluster_summary_gdf.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Berechnung der Entfernung zwischen zwei Punkten\n",
    "def distance(point1, point2):\n",
    "    return point1.distance(point2)\n",
    "\n",
    "# Berechnung der durchschnittlichen Entfernung von jedem Polygon-Centroid zu seinen Ecken\n",
    "average_distances = []\n",
    "\n",
    "# Iterieren über jede Zeile im GeoDataFrame\n",
    "for idx, row in cluster_summary_gdf.iterrows():\n",
    "    centroid = row['geometry'].centroid  # Berechnung des Zentroids des Polygons\n",
    "    polygon = row['geometry']  # Abrufen der Polygongeometrie\n",
    "\n",
    "    if isinstance(polygon, MultiPolygon):\n",
    "        average_distance = 0  # Setzen der durchschnittlichen Entfernung auf 0 für Multipolygone\n",
    "    else:\n",
    "        # Extrahieren der Koordinaten der Polygon-Ecken\n",
    "        polygon_corners = polygon.exterior.coords[:-1]  # Letzter Punkt ist der gleiche wie der erste\n",
    "        \n",
    "        # Berechnung der Entfernungen und Speichern der Ergebnisse\n",
    "        distances = [distance(centroid, Point(coord)) for coord in polygon_corners]\n",
    "        average_distance = sum(distances) / len(distances)\n",
    "        \n",
    "    average_distances.append(average_distance)\n",
    "\n",
    "# Hinzufügen der durchschnittlichen Entfernungen als neue Spalte zum GeoDataFrame\n",
    "cluster_summary_gdf['average_distance'] = average_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot der Raster mit Polygone farblich darstellen\n",
    "ax = cluster_summary_gdf.plot(cmap='viridis', legend=True, figsize=(5, 5))\n",
    "\n",
    "# Hervorhebung der Rastergrenzen\n",
    "cluster_summary_gdf.boundary.plot(ax=ax, color='black')\n",
    "\n",
    "# Titel hinzufügen\n",
    "plt.title('Bevölkerungsraster des Landkreis Würzburg')\n",
    "\n",
    "# Achsenbeschriftungen hinzufügen\n",
    "plt.xlabel('Breitengradkoordinate im CSR3035 Format')\n",
    "plt.ylabel('Längengradkoordinate im CSR3035 Format')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Projektionen\n",
    "in_proj = Proj(init='epsg:3035')  # CSR3035 (Eingangsprojektion)\n",
    "out_proj = Proj(init='epsg:4326')  # WGS84 (Ausgangsprojektion, Längen- und Breitengrade)\n",
    "\n",
    "# Funktion zum Umwandeln von CSR3035-Koordinaten in Längen- und Breitengrade\n",
    "def csr3035_to_latlon(polygon):\n",
    "    point = polygon.centroid  # Berechne den Zentroiden des Polygons\n",
    "    x_csr3035 = point.x  # X-Koordinate des Zentroiden\n",
    "    y_csr3035 = point.y  # Y-Koordinate des Zentroiden\n",
    "    lon, lat = transform(in_proj, out_proj, x_csr3035, y_csr3035)  # Transformiere die Koordinaten in Längen- und Breitengrade\n",
    "    return lon, lat  # Gebe die transformierten Koordinaten zurück\n",
    "\n",
    "# Wende die Funktion auf die Geometry-Spalte an und erstelle neue Spalten für Längen- und Breitengrade\n",
    "cluster_summary_gdf['lon'], cluster_summary_gdf['lat'] = zip(*cluster_summary_gdf['geometry'].apply(lambda point: csr3035_to_latlon(point)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifikation der Spalten, die mit 'sum_ALTER_10JG_' beginnen\n",
    "age_group_columns = [col for col in cluster_summary_gdf.columns if col.startswith('sum_ALTER_10JG_')]\n",
    "\n",
    "# Definition der neuen Altersgruppenstruktur\n",
    "new_age_groups = {\n",
    "    '0-10': 0,\n",
    "    '10-20': 3,\n",
    "    '20-30': 0,\n",
    "    '30-40': 0,\n",
    "    '40-50': 6,\n",
    "    '50-60': 8,\n",
    "    '60-70': 0,\n",
    "    '70-80': 9,\n",
    "    '80+': 0\n",
    "}\n",
    "\n",
    "# Erstellung einer Liste der neuen Altersgruppen\n",
    "age_groups_list = list(new_age_groups.keys())\n",
    "\n",
    "# Funktion zur Extraktion der Altersgruppendaten aus dem DataFrame\n",
    "def get_age_group_data(cluster_summary_gdf):\n",
    "    age_group_data = {}\n",
    "\n",
    "    # Iteration durch die ersten 9 Altersgruppen\n",
    "    for count in range(9):\n",
    "        age_group = age_groups_list[count]  # Extraktion der Altersgruppe aus der Liste\n",
    "        population = cluster_summary_gdf[age_group_columns[count]]  # Extraktion der Populationsdaten für die Altersgruppe\n",
    "        age_group_data[age_group] = population  # Speicherung der Populationsdaten in einem Dictionary\n",
    "    return age_group_data\n",
    "\n",
    "# Extraktion und Zusammenführung der Altersgruppendaten für jede Zeile im DataFrame\n",
    "age_group_data_per_cluster = cluster_summary_gdf.apply(get_age_group_data, axis=1)\n",
    "\n",
    "# Hinzufügen der extrahierten Altersgruppendaten als neue Spalte\n",
    "cluster_summary_gdf['Alter'] = age_group_data_per_cluster\n",
    "\n",
    "# Entfernung der ursprünglichen 'ALTER_10JG_' Spalten\n",
    "cluster_summary_gdf.drop(columns=age_group_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygon Geopandas speichern\n",
    "cluster_summary_gdf.to_file('./Frankfurt_Data/cluster_frankfurt.gpkg', driver='GPKG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit_python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
