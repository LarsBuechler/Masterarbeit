{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv as csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "\n",
    "from scipy.stats import poisson\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, Point, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "from pyproj import CRS\n",
    "import re\n",
    "\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bevölkerungsdaten - Zensus Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Einwohner_Data/Bevoelkerung100M.csv'\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_merkmale = [' INSGESAMT', 'ALTER_KURZ', 'GESCHLECHT', 'ALTER_10JG']\n",
    "df_einwohner = df[df['Merkmal'].isin(list_of_merkmale)]\n",
    "\n",
    "df_bezeichnungen = pd.DataFrame()\n",
    "bezeichnungen = ['Merkmal', 'Auspraegung_Text', 'Auspraegung_Code']\n",
    "df_bezeichnungen = df_einwohner[bezeichnungen]\n",
    "df_bezeichnungen.drop_duplicates(inplace=True)\n",
    "df_bezeichnungen.sort_values(by=['Merkmal', 'Auspraegung_Code'], inplace=True)\n",
    "df_bezeichnungen.reset_index(drop=True, inplace=True)\n",
    "df_bezeichnungen.to_csv('./Einwohner_Data/Bezeichnungen.csv', index=False)\n",
    "\n",
    "df_einwohner['Attribute'] = df_einwohner['Merkmal'] + '_' + df_einwohner['Auspraegung_Code'].astype(str)\n",
    "df_einwohner.drop(columns=['Merkmal', 'Auspraegung_Code', 'Auspraegung_Text', 'Gitter_ID_100m', 'Anzahl_q'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = df_einwohner.pivot(index=['Gitter_ID_100m_neu'], columns=['Attribute'], values='Anzahl').reset_index()\n",
    "pivot_df.columns.name = None\n",
    "pivot_df = pivot_df.fillna(0)\n",
    "pivot_df.rename(columns={' INSGESAMT_0': 'INSGESAMT_0'}, inplace=True)\n",
    "pivot_df.to_csv('./Einwohner_Data/Einwohner_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = pivot_df\n",
    "# Extrahiere Informationen aus der Gitter-IDs mithilfe von regulären Ausdrücken\n",
    "pattern = re.compile(r'N(\\d+)E(\\d+)')\n",
    "df_geo[['origin_n', 'origin_e']] = df_geo['Gitter_ID_100m_neu'].str.extract(pattern).astype(int)\n",
    "\n",
    "# Setze feste Werte für crs_code und resolution \n",
    "df_geo['crs_code'] = 3035\n",
    "df_geo['resolution'] = 100\n",
    "\n",
    "# Erstelle Geometrien (Quadrate) für jedes Gitter\n",
    "df_geo['geometry'] = [Polygon([(e, n), (e + 100, n), (e + 100, n - 100), (e, n - 100)]) for e, n in zip(df_geo['origin_e'], df_geo['origin_n'])]\n",
    "df_geo.to_csv('./GeoDaten/GeoDataFrame_Bevoelkerung.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df_geo, geometry='geometry', crs=CRS(f\"EPSG:{df_geo['crs_code'].iloc[0]}\"))\n",
    "\n",
    "vmin, vmax = 0, 20\n",
    "\n",
    "# Plotte das GeoDataFrame mit Farbhervorhebung der 'INSGESAMT'-Werte und angepasster Skala\n",
    "gdf.plot(column='INSGESAMT_0', cmap='viridis', legend=True, figsize=(10, 10), vmin=vmin, vmax=vmax)\n",
    "plt.title('Farbliche Hervorhebung der INSGESAMT-Werte')\n",
    "plt.show()\n",
    "gdf.to_file('./GeoDaten/Deutschland_Raster.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laden der Verwaltungsbezirke um die Raster zu filtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_bundesländer = gpd.read_file('./GeoDaten/DE_NUTS5000.gpkg')\n",
    "\n",
    "geo_würzburg = geo_bundesländer[geo_bundesländer['NUTS_NAME'].str.contains('Würzburg')]\n",
    "geo_würzburg = geo_würzburg.to_crs(gdf.crs)\n",
    "mask_overlapping = gdf.geometry.intersects(geo_würzburg.unary_union)\n",
    "gdf_würzburg = gdf[mask_overlapping]\n",
    "gdf_würzburg.to_file('./GeoDaten/Wuerzburg.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gebäudedaten und Bevölkerungsdaten mappen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_buildings = pd.read_csv('./OSM_Data/Würzburg-Buildings.csv')\n",
    "gdf_würzburg = gpd.read_file('./GeoDaten/Wuerzburg.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gebäude einem Raster zuordnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_polygon_id(point, polygons):\n",
    "    return polygons.distance(point.geometry).sort_values().index[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buildings = gpd.GeoDataFrame(df_buildings, geometry=gpd.points_from_xy(df_buildings['lon'], df_buildings['lat']), crs=CRS(\"EPSG:4326\"))\n",
    "gdf_buildings = gdf_buildings.to_crs(gdf_würzburg.crs)\n",
    "df_buildings['Raster_ID'] = -1\n",
    "\n",
    "for index, building in gdf_buildings.iterrows():\n",
    "\n",
    "    mask_overlapping = gdf_würzburg.intersects(building['geometry'])\n",
    "    \n",
    "    if any(mask_overlapping):\n",
    "        df_buildings.loc[index, 'Raster_ID'] = gdf_würzburg[mask_overlapping].index[0]\n",
    "    else:\n",
    "        df_buildings.loc[index, 'Raster_ID'] = find_nearest_polygon_id(building, gdf_würzburg)\n",
    "\n",
    "\n",
    "gdf_buildings_with_raster = gpd.GeoDataFrame(df_buildings, geometry=gdf_buildings['geometry'], crs=gdf_würzburg.crs)\n",
    "\n",
    "df_buildings.to_csv('./OSM_Data/Wuerzburg_Buildings_Raster.csv')\n",
    "gdf_buildings_with_raster.to_file('./GeoDaten/Buildings_Raster.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buildings_with_raster = gpd.read_file('./GeoDaten/Buildings_Raster.gpkg')\n",
    "gdf_würzburg = gpd.read_file('./GeoDaten/Wuerzburg.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zähle die eindeutigen Werte in der Spalte 'Spalte_Name'\n",
    "value_counts = gdf_buildings_with_raster['Raster_ID'].value_counts()\n",
    "\n",
    "# Sortiere die Ergebnisse nach den Werten\n",
    "sorted_value_counts = value_counts.sort_values(ascending = False)\n",
    "\n",
    "# Gib die sortierten Ergebnisse aus\n",
    "print(\"Sortierte Wertezählungen:\")\n",
    "print(sorted_value_counts)\n",
    "\n",
    "gdf_buildings_with_raster[gdf_buildings_with_raster['Raster_ID'] == 8207].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jedem Gebäude die Bevölkerung zuweisen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buildings_with_raster['Einwohner'] = 0\n",
    "gdf_buildings_with_raster['Alter'] = {} \n",
    "gdf_buildings_with_raster['Geschlecht'] = {} \n",
    "\n",
    "\n",
    "unique_raster_indices = gdf_buildings_with_raster['Raster_ID'].unique()\n",
    "\n",
    "for raster_index in unique_raster_indices:\n",
    "    einwohner_aus_raster = gdf_würzburg.at[raster_index, 'INSGESAMT_0']\n",
    "\n",
    "    alter_dict_raster = {\n",
    "            '0-10':  int(gdf_würzburg.at[raster_index, 'ALTER_10JG_1']),\n",
    "            '10-20': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_2']),\n",
    "            '20-30': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_3']),\n",
    "            '30-40': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_4']), \n",
    "            '40-50': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_5']), \n",
    "            '50-60': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_6']),\n",
    "            '60-70': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_7']), \n",
    "            '70-80': int(gdf_würzburg.at[raster_index, 'ALTER_10JG_8']),\n",
    "            '80+':   int(gdf_würzburg.at[raster_index, 'ALTER_10JG_9'])\n",
    "        }\n",
    "    \n",
    "    geschlecht_dict_raster = {\n",
    "            'maennlich': gdf_würzburg.at[raster_index, 'GESCHLECHT_1'],\n",
    "            'weiblich':  gdf_würzburg.at[raster_index, 'GESCHLECHT_2']\n",
    "        }\n",
    "   \n",
    "    for index, building in gdf_buildings_with_raster[gdf_buildings_with_raster['Raster_ID'] == raster_index].iterrows():\n",
    "\n",
    "        alter_dict_geb = {\n",
    "            '0-10': 0,\n",
    "            '10-20': 0,\n",
    "            '20-30': 0,\n",
    "            '30-40': 0, \n",
    "            '40-50': 0, \n",
    "            '50-60': 0,\n",
    "            '60-70': 0, \n",
    "            '70-80': 0,\n",
    "            '80+': 0\n",
    "        }\n",
    "\n",
    "        geschlecht_dict_geb = {\n",
    "            'maennlich': 0,\n",
    "            'weiblich': 0\n",
    "        }\n",
    "\n",
    "\n",
    "        if index == gdf_buildings_with_raster[gdf_buildings_with_raster['Raster_ID'] == raster_index].index.max():\n",
    "            random_einwohner = einwohner_aus_raster\n",
    "        else:\n",
    "            random_einwohner = np.random.choice(int(einwohner_aus_raster))\n",
    "\n",
    "        \n",
    "        random_einwohner = int(random_einwohner)\n",
    "        alter_count = random_einwohner\n",
    "        geschlecht_count = random_einwohner\n",
    "\n",
    "        for key in np.random.permutation(list(alter_dict_raster.keys())):\n",
    "            if alter_count == 0:\n",
    "                break\n",
    "            if alter_dict_raster[key] > 0:\n",
    "                if alter_count <= alter_dict_raster[key]:\n",
    "                    alter_dict_geb[key] = alter_count\n",
    "                    alter_dict_raster[key] -= alter_count\n",
    "                    alter_count = 0\n",
    "\n",
    "                else:\n",
    "                    alter_dict_geb[key] = alter_dict_raster[key]\n",
    "                    alter_count -= alter_dict_raster[key]\n",
    "                    alter_dict_raster[key] = 0\n",
    "        \n",
    "\n",
    "        if alter_count != 0:\n",
    "            alter_dict_geb[random.choice(list(alter_dict_geb.keys()))] += alter_count\n",
    "            \n",
    "\n",
    "\n",
    "        for key in np.random.permutation(list(geschlecht_dict_raster.keys())):\n",
    "            if geschlecht_count == 0:\n",
    "                break\n",
    "            if geschlecht_dict_raster[key] > 0:\n",
    "                if geschlecht_count <= geschlecht_dict_raster[key]:\n",
    "                    geschlecht_dict_geb[key] = geschlecht_count\n",
    "                    geschlecht_dict_raster[key] -= geschlecht_count\n",
    "                    geschlecht_count = 0\n",
    "\n",
    "                else:\n",
    "                    geschlecht_dict_geb[key] = geschlecht_dict_raster[key]\n",
    "                    geschlecht_count -= geschlecht_dict_raster[key]\n",
    "                    geschlecht_dict_raster[key] = 0\n",
    "\n",
    "\n",
    "        if geschlecht_count != 0:\n",
    "            geschlecht_dict_geb[random.choice(list(geschlecht_dict_geb.keys()))] += geschlecht_count\n",
    "\n",
    "\n",
    "        gdf_buildings_with_raster.at[index, 'Einwohner'] = random_einwohner\n",
    "        gdf_buildings_with_raster.loc[index, 'Alter'] = [alter_dict_geb]\n",
    "        gdf_buildings_with_raster.loc[index, 'Geschlecht'] = [geschlecht_dict_geb]\n",
    "        einwohner_aus_raster -= random_einwohner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buildings_with_raster['Alter'] = gdf_buildings_with_raster['Alter'].apply(json.dumps)\n",
    "gdf_buildings_with_raster['Geschlecht'] = gdf_buildings_with_raster['Geschlecht'].apply(json.dumps)\n",
    "\n",
    "gdf_buildings_with_raster.to_file('./GeoDaten/Buildings_Raster_Demographie.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jedem Einwohner einen eigenen Tupel erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_einwohner = gpd.read_file('./GeoDaten/Buildings_Raster_Demographie.gpkg')\n",
    "\n",
    "gdf_einwohner['Alter'] = gdf_einwohner['Alter'].apply(json.loads)\n",
    "gdf_einwohner['Geschlecht'] = gdf_einwohner['Geschlecht'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neue leere Liste für die aufgeschlüsselten Daten\n",
    "new_rows = []\n",
    "\n",
    "# Iteration über jede Zeile des DataFrame\n",
    "for index, row in gdf_einwohner.iterrows():\n",
    "    # Iteration über die Altersgruppen in der Spalte 'Alter'\n",
    "    \n",
    "    maennlich = int(row['Geschlecht'][0].get('maennlich', 0))\n",
    "\n",
    "    for age_group, count in row['Alter'][0].items():\n",
    "        \n",
    "        for i in range(count):\n",
    "\n",
    "            # Erstellung einer neuen Zeile für jede Altersgruppe\n",
    "            new_row = row.copy()\n",
    "            new_row['Einwohner'] = 1  # Aktualisierung der Einwohnerzahl für die Altersgruppe\n",
    "            new_row['Alter'] = age_group  # Aktualisierung der Altersgruppe\n",
    "            if maennlich != 0:\n",
    "                new_row['Geschlecht'] = 'maennlich'\n",
    "                maennlich -= 1\n",
    "\n",
    "            else:\n",
    "                new_row['Geschlecht'] = 'weiblich'\n",
    "\n",
    "\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "# Neuen DataFrame erstellen\n",
    "new_df = pd.DataFrame(new_rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jedem Einwohner eine Nachfrage zuweisen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_loaded = gpd.read_file('./GeoDaten/Buildings_Raster_Demographie.gpkg')\n",
    "\n",
    "gdf_loaded['Alter'] = gdf_loaded['Alter'].apply(json.loads)\n",
    "gdf_loaded['Geschlecht'] = gdf_loaded['Geschlecht'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_bundesländer = gpd.read_file('./GeoDaten/DE_NUTS5000.gpkg')\n",
    "\n",
    "geo_würzburg = geo_bundesländer[geo_bundesländer['NUTS_NAME'].str.contains('Würzburg')]\n",
    "geo_würzburg = geo_würzburg.to_crs(gdf_würzburg.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wahrscheinlichkeitsverteilung\n",
    "probabilities = {\n",
    "    '0-10': 0.01,\n",
    "    '10-20': 0.02,\n",
    "    '20-30': 0.03,\n",
    "    '30-40': 0.04,\n",
    "    '40-50': 0.07,\n",
    "    '50-60': 0.09,\n",
    "    '60-70': 0.1,\n",
    "    '70-80': 0.15,\n",
    "    '80+': 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = gdf_loaded\n",
    "\n",
    "for index, row in df_data.iterrows():\n",
    "    age_groups_list = row['Alter']\n",
    "\n",
    "    if age_groups_list:\n",
    "        age_groups_dict = age_groups_list[0]\n",
    "        \n",
    "        for age_group, probability in probabilities.items():\n",
    "            if age_group in age_groups_dict:\n",
    "                num_residents = age_groups_dict[age_group]\n",
    "                num_selected = np.random.binomial(num_residents, probability)\n",
    "                df_data.at[index, 'Nachfrage'] += num_selected\n",
    "            else:\n",
    "                print(f\"Die Altersgruppe '{age_group}' ist nicht im Dictionary enthalten.\")\n",
    "    else:\n",
    "        print(\"Die Liste 'Alter' ist leer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['Alter'] = gdf_buildings_with_raster['Alter'].apply(json.dumps)\n",
    "df_data['Geschlecht'] = gdf_buildings_with_raster['Geschlecht'].apply(json.dumps)\n",
    "\n",
    "df_data.to_file('./Nachfrage/Wuerzburg_Nachfrage.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = df_data['Nachfrage'].value_counts()\n",
    "sorted_value_counts = value_counts.sort_values(ascending = False)\n",
    "\n",
    "print(\"Sortierte Wertezählungen:\")\n",
    "print(sorted_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot des GeoDataFrames mit hervorgehobener \"Nachfrage\"-Spalte\n",
    "ax = df_data.plot(column='Nachfrage', cmap='viridis', legend=True, figsize=(4, 4))\n",
    "\n",
    "# Legende anzeigen\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haushalte-Daten Zensus Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './Einwohner_Data/Haushalte100m.csv'\n",
    "df_haushalte = pd.read_csv(file_path, encoding='ISO-8859-1', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bezeichnungen = pd.DataFrame()\n",
    "bezeichnungen = ['Merkmal', 'Auspraegung_Text', 'Auspraegung_Code']\n",
    "df_bezeichnungen = df_haushalte[bezeichnungen]\n",
    "df_bezeichnungen.drop_duplicates(inplace=True)\n",
    "df_bezeichnungen.sort_values(by=['Merkmal', 'Auspraegung_Code'], inplace=True)\n",
    "df_bezeichnungen.reset_index(drop=True, inplace=True)\n",
    "df_bezeichnungen.to_csv('./Einwohner_Data/Bezeichnungen_Haushalte.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_haushalte['Attribute'] = df_haushalte['Merkmal'] + '_' + df_haushalte['Auspraegung_Text'].astype(str)\n",
    "df_haushalte.drop(columns=['Merkmal', 'Auspraegung_Code', 'Auspraegung_Text', 'Gitter_ID_100m', 'Anzahl_q'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df_haushalte = df_haushalte.pivot(index=['Gitter_ID_100m_neu'], columns=['Attribute'], values='Anzahl').reset_index()\n",
    "pivot_df_haushalte.columns.name = None\n",
    "pivot_df_haushalte = pivot_df_haushalte.fillna(0)\n",
    "pivot_df_haushalte.to_csv('./Einwohner_Data/Haushalte_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = pivot_df_haushalte\n",
    "# Extrahiere Informationen aus der Gitter-IDs mithilfe von regulären Ausdrücken\n",
    "pattern = re.compile(r'N(\\d+)E(\\d+)')\n",
    "df_geo[['origin_n', 'origin_e']] = df_geo['Gitter_ID_100m_neu'].str.extract(pattern).astype(int)\n",
    "\n",
    "# Setze feste Werte für crs_code und resolution \n",
    "df_geo['crs_code'] = 3035\n",
    "df_geo['resolution'] = 100\n",
    "\n",
    "# Erstelle Geometrien (Quadrate) für jedes Gitter\n",
    "df_geo['geometry'] = [Polygon([(e, n), (e + 100, n), (e + 100, n - 100), (e, n - 100)]) for e, n in zip(df_geo['origin_e'], df_geo['origin_n'])]\n",
    "df_geo.to_csv('./GeoDaten/GeoDataFrame_Haushalte.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_haushalte = gpd.GeoDataFrame(df_geo, geometry='geometry', crs=CRS(f\"EPSG:{df_geo['crs_code'].iloc[0]}\"))\n",
    "\n",
    "vmin, vmax = 0, 20\n",
    "\n",
    "# Plotte das GeoDataFrame mit Farbhervorhebung der 'INSGESAMT'-Werte und angepasster Skala\n",
    "gdf_haushalte.plot(column='INSGESAMT_Einheiten insgesamt', cmap='viridis', legend=True, figsize=(10, 10), vmin=vmin, vmax=vmax)\n",
    "plt.title('Farbliche Hervorhebung der INSGESAMT-Werte')\n",
    "plt.show()\n",
    "gdf_haushalte.to_file('./GeoDaten/Deutschland_Raster_Haushalte.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nachfrage-Wahrscheinlichkeitsverteilung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nachfragestatistik - Apothekenbesuch und dauerhafte Einnahme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequent von Apothekenbesuchen nach Altersverteilung\n",
    "\"Mehrmals in der Woche, Etwa einmal in der Woche, Zwei- bis dreimal im Monat, Einmal im Monat, Etwa einmal im Vierteljahr, Seltener, Nie, so gut wie nie\"\n",
    "apotheken_besuch = {\n",
    "    '14-19': [0.002, 0.005, 0.023, 0.053, 0.18, 0.305, 0.432],\n",
    "    '20-29': [0.003, 0.009, 0.052, 0.131, 0.282, 0.326, 0.197],\n",
    "    '30-39': [0.006, 0.013, 0.098, 0.198, 0.287, 0.275, 0.123],\n",
    "    '40-49': [0.009, 0.014, 0.105, 0.21, 0.297, 0.258, 0.106],\n",
    "    '50-59': [0.008, 0.019, 0.116, 0.252, 0.308, 0.209, 0.087],\n",
    "    '60-69': [0.005, 0.037, 0.173, 0.277, 0.278, 0.159, 0.071],\n",
    "    '70+':   [0.011, 0.065, 0.252, 0.336, 0.196, 0.064, 0.075]\n",
    "}\n",
    "\n",
    "\n",
    "# Dauerhafte Einnahme von Medikamenten\n",
    "'keine, ein bis zwei, drei, vier, fünf oder mehr'\n",
    "dauerhafte_einnahme = {\n",
    "    'Maenner': [0.51, 0.24, 0.08, 0.06, 0.11],\n",
    "    'Frauen':  [0.41, 0.35, 0.09, 0.06, 0.09],\n",
    "    '18-29':   [0.66, 0.30, 0.02, 0.01, 0.01],\n",
    "    '30-49':   [0.59, 0.31, 0.05, 0.02, 0.03],\n",
    "    '50-69':   [0.37, 0.31, 0.12, 0.08, 0.12],\n",
    "    '70+':     [0.22, 0.23, 0.16, 0.14, 0.25]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sum_one(data_dict):\n",
    "    for key, values in data_dict.items():\n",
    "        total = sum(values)\n",
    "        if not np.isclose(total, 1):\n",
    "            print(f\"Die Summe der Werte für {key} beträgt nicht 1, sondern {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_sum_one(apotheken_besuch)\n",
    "check_sum_one(dauerhafte_einnahme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiederholte Nachfrage und Behandlungsdauer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packungsgrößen N1, N2, N3\n",
    "    # N1: 10 Tage\n",
    "    # N2: 30 Tage\n",
    "    # N3: 100 Tage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transportkapazität"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drohnenlogistik Unternehmen und Drohnenparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 größten Drohnenlogistik Unternehmen:\n",
    "    # Drone Delivery Canada\n",
    "    # Amazon.com, Inc.\n",
    "    # Matternet\n",
    "    # DHL\n",
    "    # Zipline International Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drone Delivery Canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparrow\n",
    "#     Max Range 20 km\n",
    "#     Max Speed 60 km/h\n",
    "#     Max Payload 4 kg\n",
    "#     MTOW 25 kg\n",
    "#     Aircraft Type Rotorcraft\n",
    "#     Powerplant 8 Electric Motors\n",
    "#     Navigation GPS-based\n",
    "#     Delivery Options Land - Drop Ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canary\n",
    "#     Max Range 20 km\n",
    "#     Max Speed 72 km/h\n",
    "#     Max Payload 4.5 kg\n",
    "#     MTOW 25 kg\n",
    "#     Aircraft Type Rotorcraft\n",
    "#     Powerplant 8 Electric Motors\n",
    "#     Navigation GPS-based\n",
    "#     Delivery Options Land - Drop Ship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon.com, Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prime Air MK27-2\n",
    "#     Max Range 25 km\n",
    "#     Max Speed 80 km/h\n",
    "#     Max Payload 2.26 kg\n",
    "#     MTOW 25 kg\n",
    "#     Max Altitude 122 m\n",
    "#     Weather resistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prime Air MK30\n",
    "#     ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matternet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M2 Drone \n",
    "    # Max Range 20 km\n",
    "    # Max Speed 50 km/h\n",
    "    # Max Payload 2 kg\n",
    "    # Max Altitude 120 m\n",
    "    # MTOW 9.5 kg\n",
    "    # Retail Price 7,499.00$\n",
    "    # Aircraft type Quadcopter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DHL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipline International Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P1\n",
    "#   Max Range 193 km\n",
    "#   Max Speed 105 km/h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warehouse Location Problem Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering der Rastereinheiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_wuerzburg = gpd.read_file('./GeoDaten/Wuerzburg.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot der Raster mit Polygone farblich darstellen\n",
    "ax = gdf_wuerzburg.plot(cmap='viridis', legend=True, figsize=(5, 5))\n",
    "\n",
    "# Hervorhebung der Rastergrenzen\n",
    "gdf_wuerzburg.boundary.plot(ax=ax, color='black')\n",
    "\n",
    "# Titel hinzufügen\n",
    "plt.title('Bevölkerungsraster des Landkreis Würzburg')\n",
    "\n",
    "# Achsenbeschriftungen hinzufügen\n",
    "plt.xlabel('Breitengradkoordinate im CSR3035 Format')\n",
    "plt.ylabel('Längengradkoordinate im CSR3035 Format')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren der Polygon-Koordinaten als Features für das Clustering\n",
    "X = np.array(gdf_wuerzburg.geometry.apply(lambda polygon: [polygon.centroid.x, polygon.centroid.y]).tolist())\n",
    "\n",
    "# DBSCAN-Clustering durchführen\n",
    "dbscan = DBSCAN(eps=100, min_samples=4)  # Anpassen von eps und min_samples je nach Bedarf\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Fügen Sie die Cluster-Zuordnung als neue Spalte zum GeoDataFrame hinzu\n",
    "gdf_wuerzburg['cluster'] = dbscan.labels_\n",
    "\n",
    "# Plotten der Polygone mit Cluster-Färbung\n",
    "ax = gdf_wuerzburg.plot(column='cluster', cmap='tab20', legend=True, figsize=(5, 5))\n",
    "plt.title('Clustering der Polygone mit DBSCAN')\n",
    "plt.xlabel('X-Achse')\n",
    "plt.ylabel('Y-Achse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alle eindeutigen Cluster-Werte und ihre Anzahl der Polygone ausgeben\n",
    "cluster_counts = gdf_wuerzburg['cluster'].value_counts().sort_index()\n",
    "print(\"Cluster-Werte und ihre Anzahl der Polygone:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} Polygone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extrahieren der eindeutigen Cluster-Bezeichnungen\n",
    "# unique_clusters = gdf_wuerzburg['cluster'].unique()\n",
    "\n",
    "# # Plotten der Polygone für jedes Cluster separat\n",
    "# for cluster_label in unique_clusters:\n",
    "#     if cluster_label == -1:  # Auslassung von Rauschpunkten (Clusterlabel -1)\n",
    "#         continue\n",
    "#     cluster_gdf = gdf_wuerzburg[gdf_wuerzburg['cluster'] == cluster_label]\n",
    "#     ax = cluster_gdf.plot(color='blue', alpha=0.5, figsize=(10, 10))\n",
    "#     plt.title(f'Cluster {cluster_label}')\n",
    "#     plt.xlabel('X-Achse')\n",
    "#     plt.ylabel('Y-Achse')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotten der Rauschpunkte (Clusterlabel -1)\n",
    "noise_points = gdf_wuerzburg[gdf_wuerzburg['cluster'] == -1]\n",
    "ax = noise_points.boundary.plot(color='red', alpha=0.5, figsize=(5, 5))\n",
    "plt.title('Noise')\n",
    "plt.xlabel('X-Achse')\n",
    "plt.ylabel('Y-Achse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_sum = 'INSGESAMT_0'\n",
    "sum_of_column = noise_points[column_to_sum].sum()\n",
    "\n",
    "print('Anzahl der Raster, die als Noise erkannt wurden: {}'.format(len(noise_points)))\n",
    "print(\"Sum of column '{}' in noise points: {}\".format(column_to_sum, sum_of_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gruppierung nach dem Cluster und Aggregierung der Punkte in jedem Cluster\n",
    "cluster_summary = gdf_wuerzburg.groupby('cluster').agg(\n",
    "    Gitter_ID_100m_neu=('Gitter_ID_100m_neu', 'first'),\n",
    "    count=('cluster', 'size'),  # Anzahl der Polygone in jedem Cluster\n",
    "    sum_INSGESAMT_0=('INSGESAMT_0', 'sum'),  \n",
    "    sum_ALTER_10JG_1=('ALTER_10JG_1', 'sum'), \n",
    "    sum_ALTER_10JG_2=('ALTER_10JG_2', 'sum'), \n",
    "    sum_ALTER_10JG_3=('ALTER_10JG_3', 'sum'), \n",
    "    sum_ALTER_10JG_4=('ALTER_10JG_4', 'sum'), \n",
    "    sum_ALTER_10JG_5=('ALTER_10JG_5', 'sum'), \n",
    "    sum_ALTER_10JG_6=('ALTER_10JG_6', 'sum'), \n",
    "    sum_ALTER_10JG_7=('ALTER_10JG_7', 'sum'), \n",
    "    sum_ALTER_10JG_8=('ALTER_10JG_8', 'sum'), \n",
    "    sum_ALTER_10JG_9=('ALTER_10JG_9', 'sum'),\n",
    "    sum_GESCHLECHT_1=('GESCHLECHT_1', 'sum'),\n",
    "    sum_GESCHLECHT_2=('GESCHLECHT_2', 'sum'),\n",
    "    crs_code=('crs_code', 'first'),\n",
    "    resolution=('resolution', 'first'),\n",
    "    origin_n=('origin_n', 'first'),\n",
    "    origin_e=('origin_e', 'first'),\n",
    "    geometry=('geometry', lambda x: unary_union(x))  # Vereinigung der Polygone in jedem Cluster\n",
    ") \n",
    "\n",
    "# Umwandeln des aggregierten DataFrames in ein GeoDataFrame\n",
    "cluster_summary_gdf = gpd.GeoDataFrame(cluster_summary, geometry='geometry', crs=gdf_wuerzburg.crs)\n",
    "cluster_summary_gdf = cluster_summary_gdf.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zähler für Multipolygone initialisieren\n",
    "multipolygon_count = 0\n",
    "\n",
    "# Schleife über die Geometrien in der Spalte 'geometry_column'\n",
    "for geometry in cluster_summary_gdf['geometry']:\n",
    "    # Überprüfen, ob die Geometrie ein Multipolygon ist\n",
    "    if isinstance(geometry, MultiPolygon):\n",
    "        multipolygon_count += 1\n",
    "\n",
    "# Anzeigen der Anzahl der Multipolygone\n",
    "print(\"Anzahl der Multipolygone:\", multipolygon_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Berechnung der Entfernung zwischen zwei Punkten\n",
    "def distance(point1, point2):\n",
    "    return point1.distance(point2)\n",
    "\n",
    "cluster_summary_gdf['centroid'] = cluster_summary_gdf['geometry'].centroid\n",
    "\n",
    "# Berechnung der durchschnittlichen Entfernung von jedem Polygon-Centroid zu seinen Ecken\n",
    "average_distances = []\n",
    "for idx, row in cluster_summary_gdf.iterrows():\n",
    "    centroid = row['centroid']\n",
    "    polygon = row['geometry']\n",
    "\n",
    "    if isinstance(polygon, MultiPolygon):\n",
    "        average_distance = 0  # Setzen Sie die durchschnittliche Entfernung auf 0 für Multipolygone\n",
    "    else:\n",
    "        # Extrahieren der Koordinaten der Polygon-Ecken\n",
    "        polygon_corners = polygon.exterior.coords[:-1]  # Letzter Punkt ist der gleiche wie der erste\n",
    "        # Berechnung der Entfernungen und Speichern der Ergebnisse\n",
    "        distances = [distance(centroid, Point(coord)) for coord in polygon_corners]\n",
    "        average_distance = sum(distances) / len(distances)\n",
    "        \n",
    "    average_distances.append(average_distance)\n",
    "\n",
    "# Hinzufügen der durchschnittlichen Entfernungen als neue Spalte zum GeoDataFrame\n",
    "cluster_summary_gdf['average_distance'] = average_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot der Raster mit Polygone farblich darstellen\n",
    "ax = cluster_summary_gdf.plot(cmap='viridis', legend=True, figsize=(5, 5))\n",
    "\n",
    "# Hervorhebung der Rastergrenzen\n",
    "cluster_summary_gdf.boundary.plot(ax=ax, color='black')\n",
    "\n",
    "# Titel hinzufügen\n",
    "plt.title('Bevölkerungsraster des Landkreis Würzburg')\n",
    "\n",
    "# Achsenbeschriftungen hinzufügen\n",
    "plt.xlabel('Breitengradkoordinate im CSR3035 Format')\n",
    "plt.ylabel('Längengradkoordinate im CSR3035 Format')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid Geopandas speichern\n",
    "save_gdf = cluster_summary_gdf\n",
    "save_gdf = save_gdf.drop(columns=['geometry'])\n",
    "save_gdf = save_gdf.rename(columns={'centroid': 'geometry'})\n",
    "save_gdf.to_file('./WLP/centroid_wuerzburg.gpkg', driver='GPKG')\n",
    "\n",
    "# Polygon Geopandas speichern\n",
    "cluster_summary_gdf = cluster_summary_gdf.drop(columns=['centroid'])\n",
    "cluster_summary_gdf = cluster_summary_gdf[cluster_summary_gdf['cluster'] != -1].reset_index(drop=True)\n",
    "cluster_summary_gdf.to_file('./WLP/cluster_wuerzburg.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping der Nachfrage auf die Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anhand der Packungsgrößen berechnet (N1 - N3), aufsummiert auf ein Jahr\n",
    "# 2022 1.405 Mio. Packungen abgegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Nachfrage von 100-1000 je Cluster um Daten zu simulieren\n",
    "nachfrage_cluster = gpd.read_file('./WLP/centroid_wuerzburg.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_group_columns = [col for col in nachfrage_cluster.columns if col.startswith('sum_ALTER_10JG_')]\n",
    "\n",
    "new_age_groups = { '0-10': 0,\n",
    "                    '10-20': 3,\n",
    "                    '20-30': 0,\n",
    "                    '30-40': 0,\n",
    "                    '40-50': 6,\n",
    "                    '50-60': 8,\n",
    "                    '60-70': 0,\n",
    "                    '70-80': 9,\n",
    "                    '80+': 0}\n",
    "\n",
    "age_groups_list = list(new_age_groups.keys())\n",
    "\n",
    "\n",
    "def get_age_group_data(nachfrage_cluster):\n",
    "    age_group_data = {}\n",
    "\n",
    "    for count in range(9):\n",
    "        age_group =  age_groups_list[count] # Extract age group from column name\n",
    "        population = nachfrage_cluster[age_group_columns[count]]\n",
    "        age_group_data[age_group] = population\n",
    "    return age_group_data\n",
    "\n",
    "\n",
    "# Extract and merge age group data\n",
    "age_group_data_per_cluster = nachfrage_cluster.apply(get_age_group_data, axis=1)\n",
    "\n",
    "nachfrage_cluster['Alter'] = age_group_data_per_cluster\n",
    "\n",
    "# Remove the original 'ALTER_10JG_' columns\n",
    "nachfrage_cluster.drop(columns=age_group_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'nachfrage_cluster' is your GeoPandas DataFrame\n",
    "probabilities = {\n",
    "    '0-10': 0.01,\n",
    "    '10-20': 0.02,\n",
    "    '20-30': 0.03,\n",
    "    '30-40': 0.04,\n",
    "    '40-50': 0.07,\n",
    "    '50-60': 0.09,\n",
    "    '60-70': 0.1,\n",
    "    '70-80': 0.15,\n",
    "    '80+': 0.2\n",
    "}\n",
    "\n",
    "def calculate_demand(row):\n",
    "    total_demand = 0\n",
    "    for age_group, count in row['Alter'].items():\n",
    "        lambda_value = count * probabilities[age_group]\n",
    "        total_demand += poisson.rvs(lambda_value)\n",
    "    return total_demand * 365\n",
    "\n",
    "nachfrage_cluster['nachfrage'] = nachfrage_cluster.apply(calculate_demand, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung des Gesamtbedarfs ohne die Noise-Punkte\n",
    "total_demand = nachfrage_cluster[nachfrage_cluster['cluster'] != -1]['nachfrage'].sum()\n",
    "\n",
    "# Summe der Polygone in jedem Cluster (ohne Noise)\n",
    "bevoelkerung_sum = nachfrage_cluster[nachfrage_cluster['cluster'] != -1].groupby('cluster')['sum_INSGESAMT_0'].sum()\n",
    "\n",
    "# Wert der Reihe mit den Noise-Punkten\n",
    "noise_demand = nachfrage_cluster[nachfrage_cluster['cluster'] == -1]['nachfrage'].iloc[0]\n",
    "\n",
    "# Verteilung des Noise-Bedarfs auf die anderen Cluster anteilig zur Anzahl der Polygone\n",
    "for cluster in bevoelkerung_sum.index:\n",
    "    cluster_demand = noise_demand * (bevoelkerung_sum[cluster] / bevoelkerung_sum.sum())\n",
    "    nachfrage_cluster.loc[nachfrage_cluster['cluster'] == cluster, 'nachfrage'] += np.round(cluster_demand)\n",
    "\n",
    "# Entfernen der Reihe mit dem 'cluster'-Wert von -1\n",
    "nachfrage_cluster = nachfrage_cluster[nachfrage_cluster['cluster'] != -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "nachfrage_cluster.to_file('./WLP/nachfrage_wuerzburg.gpkg', driver ='GPKG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterarbeit_python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
